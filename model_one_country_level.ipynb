{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1b: Single-country policy simulation\n",
    "\n",
    "The objective of this model-based simulation is to analyse the impact of policy, technology, and commodity changes on consumer price inflation in selected countries. The simulation environment is learnt from real data, after which simulations using synthetic data are used to do policy analysis by manipulating a number of selected variables such as government debt, cellular subscription, gdp growth, and real interest rates in the synthetic data. A secondary purpose of the simulation model is to identify and map the interactions between world-level and country-level indicator variables.\n",
    "\n",
    "#### Features\n",
    "------------\n",
    "\n",
    "Human and technological development indicator timeseries for a country x.\n",
    "\n",
    "#### Labels\n",
    "----------\n",
    "\n",
    "Consumer price inflation levels.\n",
    "\n",
    "#### Training\n",
    "------------\n",
    "\n",
    "Training is done on a feature - single country basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:20,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'France'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and combine the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('features/m_one/%s_features.csv' % country, sep=';', header=0)\n",
    "labels_df = pd.read_csv('features/m_one/labels_interpolated.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([features_df, labels_df.drop(columns=['date'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "[sns.lineplot(x='date', y=c, markers=True, ax=ax, label=c, data=combined_df) for c in list([country, 'lending interest rate', 'real interest rate', 'inflation', 'gross domestic savings', 'government debt service'])]\n",
    "\n",
    "xticks=ax.xaxis.get_major_ticks()\n",
    "for i in range(len(xticks)):\n",
    "    if i % 12 == 1:\n",
    "        xticks[i].set_visible(True)\n",
    "    else:\n",
    "        xticks[i].set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(combined_df['date'], rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the country features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_df = combined_df[['date', 'bank capital to assets ratio', 'bank nonperforming loans', 'cereal yield',\n",
    "                               'energy imports', 'food exports', 'high-tech exports', 'inflation',\n",
    "                               'lending interest rate', 'life expectancy', 'population density', 'real interest rate',\n",
    "                               'broad money', 'exports of goods and services', 'gross domestic savings',\n",
    "                               'high-tech value added', 'household consumption expenditure',\n",
    "                               'imports of goods and services', 'listed companies', 'manufacturing value added',\n",
    "                               'r and d spend', 'services trade', 'trade', 'government debt service',\n",
    "                               'government interest payments external debt', 'government tax revenue', 'birth deaths',\n",
    "                               'broadband subscriptions', 'electricity access', 'co2 emissions',\n",
    "                               'electricity consumption', 'mobile subscriptions', 'newborns', 'overweight',\n",
    "                               'rural population', 'urban population', country]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_df.to_csv('features/m_one/combined_country_level_%s.csv' % country.lower(), sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_df['label'] = base_feature_df[country].shift(periods=1)\n",
    "base_df = base_feature_df.drop(country, axis=1).fillna(0.00);\n",
    "base_df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = len(base_df)\n",
    "num_cols = len(base_df.columns)\n",
    "num_features = len(base_df.columns) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model iterations\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration 0\n",
    "\n",
    "**ARIMA** fitted on the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_params = {\n",
    "    'lag': 4,\n",
    "    'difference': 2,\n",
    "    'moving_average': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exo_cols = ['co2 emissions', 'rural population', 'electricity consumption', 'lending interest rate']\n",
    "\n",
    "ar_endo = base_df['label'].values\n",
    "ar_exo = base_df[exo_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "[sns.lineplot(x='date', y=c, markers=True, ax=ax, label='cpi in %s' % c, data=combined_df) for c in list([country])]\n",
    "\n",
    "xticks=ax.xaxis.get_major_ticks()\n",
    "for i in range(len(xticks)):\n",
    "    if i % 12 == 1:\n",
    "        xticks[i].set_visible(True)\n",
    "    else:\n",
    "        xticks[i].set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(combined_df['date'], rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ar_endo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_endo_train, ar_endo_test = ar_endo[0:550], ar_endo[551:696]\n",
    "ar_exo_train, ar_exo_test = ar_exo[0:550], ar_exo[551:696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_exo_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = ARIMA(ar_endo, order=(ar_params['lag'], ar_params['difference'], ar_params['moving_average']), exog=ar_exo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fitted = arima.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_fitted.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the ARIMA predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "obs = []\n",
    "hist = [x for x in ar_endo_train]\n",
    "exo_hist = [x for x in ar_exo_train]\n",
    "\n",
    "for t in range(len(ar_endo_test)):\n",
    "    m = ARIMA(hist, order=(ar_params['lag'], ar_params['difference'], ar_params['moving_average']))\n",
    "    m_fit = m.fit()\n",
    "    yhat = m_fit.forecast()[0][0]\n",
    "    preds.append(yhat)\n",
    "    hist.append(ar_endo_test[t])\n",
    "    exo_hist.append(ar_exo_test[t])\n",
    "    if t % 50 == 0:\n",
    "        print('obs: %s, pred: %s' % (ar_endo_test[t], yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list(map(lambda x: 0.00 if np.isnan(x) else x, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(ar_endo_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ar_endo_test)\n",
    "plt.plot(preds, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration 1\n",
    "\n",
    "**Multivariate LSTM** fitted on the real data, see https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "- Activation function: Leaky ReLU.\n",
    "- Loss function: mean squared error.\n",
    "- Optimizer: adam.\n",
    "- Num observations source dataset: 684 (using lagshift, 1960-2016 inclusive monthly)\n",
    "- Num sequences (@ sequence length 6): 116.\n",
    "- Batch size: 4-8 sequences (although `size=48` would lead to more stable training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Dense, LeakyReLU, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_params = {\n",
    "   'sequence_length': 4,\n",
    "   'batch_size': 8,\n",
    "   'num_epochs': 600,\n",
    "   'num_units': 128,\n",
    "   'lrelu_alpha': 0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "labels = []\n",
    "\n",
    "for i in range(int(num_obs / lstm_params['sequence_length'])):\n",
    "    labels_df = base_df['label']\n",
    "    labels.append(labels_df[i:(i+lstm_params['sequence_length'])].values[-1:])\n",
    "    features.append(base_df[i:(i+lstm_params['sequence_length'])].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train_X = np.asarray(features[0:100])\n",
    "lstm_train_X = lstm_train_X.reshape((lstm_train_X.shape[0], lstm_params['sequence_length'], num_cols))\n",
    "lstm_train_y = np.asarray(labels[0:100])\n",
    "lstm_train_y = lstm_train_y.reshape((lstm_train_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test_X = np.asarray(features[100:])\n",
    "lstm_test_X = lstm_test_X.reshape((lstm_test_X.shape[0], lstm_params['sequence_length'], num_cols))\n",
    "lstm_test_y = np.asarray(labels[100:])\n",
    "lstm_test_y = lstm_test_y.reshape((lstm_test_y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray(features)\n",
    "X = X.reshape((X.shape[0], lstm_params['sequence_length'], num_cols))\n",
    "y = np.asarray(labels)\n",
    "y = y.reshape((y.shape[0], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X: %s, y: %s' % (X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(lstm_params['num_units'], input_shape=(lstm_params['sequence_length'], num_cols)))\n",
    "model.add(Dense(1, activation=LeakyReLU(alpha=lstm_params['lrelu_alpha'])))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss', mode='min', patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = model.fit(lstm_train_X, lstm_train_y, epochs=lstm_params['num_epochs'],\n",
    "                      batch_size=lstm_params['batch_size'], callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_run.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(lstm_test_X, lstm_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(lstm_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(lstm_test_y, label='observed')\n",
    "plt.plot(yhat, label='predicted')\n",
    "plt.legend()\n",
    "plt.title('Observed versus predicted values for consumer price inflation in %s' % country)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rmse: %s\\nmean observed: %s\\nmean predicted: %s' % (np.sqrt(mean_squared_error(lstm_test_y, yhat)),\n",
    "                                                           np.mean(lstm_test_y), np.mean(yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration 2\n",
    "--------------------\n",
    "\n",
    "**GAN** to generate training data, **LSTM** trained on generated data validated on the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional GAN for policy-constrained timeseries generation\n",
    "\n",
    "See https://arxiv.org/pdf/1706.02633.pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_df = base_df[['label', 'inflation']]\n",
    "gan_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_cols = gan_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_params = {\n",
    "   'num_epochs': 800,\n",
    "   'save_interval': 100,\n",
    "   'sequence_length': 6,\n",
    "   'num_variables': gan_cols,\n",
    "   'batch_size': 64,\n",
    "   'lr': 0.0001 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = {\n",
    "   'noise_sigma': 0.3,\n",
    "   'lstm_units': 128,\n",
    "   'lstm_dropout': 0.4,\n",
    "   'gru_units': 64,\n",
    "   'lr': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_params = {\n",
    "   'bi_lstm_units': 256,\n",
    "   'dropout_rate': 0.4,\n",
    "   'lr': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN input sequences\n",
    "\n",
    "The collated World Bank and IMF data used as input for the data generator and to validate the model trained on generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_features = []\n",
    "gan_labels = []\n",
    "\n",
    "for i in range(int(num_obs / gan_params['sequence_length'])):\n",
    "    gan_labels_df = gan_df['label']\n",
    "    gan_labels.append(gan_labels_df[i:(i+gan_params['sequence_length'])].values[-1:])\n",
    "    gan_features.append(gan_df[i:(i+gan_params['sequence_length'])].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = np.asarray(gan_features)\n",
    "real = real.reshape((real.shape[0], gan_params['sequence_length'], gan_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GaussianNoise, LSTM, Dropout, BatchNormalization, Dense, LocallyConnected2D, GRU, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(params):\n",
    "    gshape = params['sequence_length'], params['num_variables']\n",
    "    inputs = Input(shape=(gshape))\n",
    "    \n",
    "    e = Sequential(name='encoder')\n",
    "    e.add(LSTM(params['lstm_units'], input_shape=(gshape), return_sequences=True))\n",
    "    e.add(Dropout(params['lstm_dropout']))\n",
    "    e.add(GaussianNoise(stddev=params['noise_sigma']))\n",
    "    e.add(BatchNormalization(axis=2, momentum=0.8, epsilon=0.01))\n",
    "    e.add(Dense(params['num_variables'], activation='relu'))\n",
    "    e.summary()\n",
    "    \n",
    "    return Model(inputs, e(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder({**gan_params, **generator_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(params):\n",
    "    gshape = params['sequence_length'], params['num_variables']\n",
    "    inputs = Input(shape=(gshape))\n",
    "    \n",
    "    g = Sequential(name='generator')\n",
    "    g.add(GRU(params['gru_units'], input_shape=(gshape), return_sequences=True))\n",
    "    g.add(Dense(params['num_variables'], activation='softmax'))\n",
    "    g.add(Reshape(target_shape=(gshape)))\n",
    "    g.summary()\n",
    "    \n",
    "    return Model(inputs, g(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator({**gan_params, **generator_params})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM, Dense, concatenate, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(params):\n",
    "    dshape = params['sequence_length'], params['num_variables']\n",
    "    batch_shape = params['batch_size'], params['sequence_length'], params['num_variables']\n",
    "    \n",
    "    real = Input(shape=(dshape))\n",
    "    generated = Input(shape=(dshape))\n",
    "    inputs = concatenate([generated, real], axis=1)\n",
    "    \n",
    "    d = Sequential(name='discriminator')\n",
    "    d.add(Bidirectional(LSTM(params['bi_lstm_units']), batch_input_shape=(batch_shape)))\n",
    "    d.add(Dropout(params['dropout_rate']))\n",
    "    d.add(Dense(1, activation='sigmoid'))\n",
    "    d.summary()\n",
    "    return Model([generated, real], d(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator({**gan_params, **discriminator_params})\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(lr=discriminator_params['lr']), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN\n",
    "\n",
    "Bidirectional generative adversarial network, viz https://arxiv.org/abs/1605.09782."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(encoder, generator, discriminator, params):\n",
    "    ganshape = params['sequence_length'], params['num_variables']\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    noise = Input(shape=(ganshape))\n",
    "    generated = generator(noise)\n",
    "    \n",
    "    data = Input(shape=(ganshape))\n",
    "    encoded = encoder(data)\n",
    "    \n",
    "    fake = discriminator([noise, generated])\n",
    "    real = discriminator([encoded, data])\n",
    "    \n",
    "    gan = Model([noise, data], [fake, real], name='gan')\n",
    "    gan.summary()\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = build_gan(encoder, generator, discriminator, gan_params)\n",
    "gan.compile(loss=['kullback_leibler_divergence', 'kullback_leibler_divergence'], \n",
    "            optimizer=Adam(lr=generator_params['lr']), metrics=['mse', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(real, batch_size, params):\n",
    "    g_metrics = []\n",
    "    d_real_metrics = []\n",
    "    d_synth_metrics = []\n",
    "    \n",
    "    reals = np.ones(batch_size)\n",
    "    synths = np.zeros(batch_size)\n",
    "    \n",
    "    for i in range(params['num_epochs']):\n",
    "        # create input of real and synthetic data\n",
    "        random_index = np.random.randint(0, len(real) - batch_size)\n",
    "        half_real = real[random_index:int(random_index + batch_size)]\n",
    "        half_synth = np.random.normal(-1.0, 1.0, size=[batch_size, params['sequence_length'], real.shape[2]])\n",
    "        \n",
    "        # apply generator and encoder\n",
    "        generated = generator.predict(half_synth)\n",
    "        encoded = encoder.predict(half_real)\n",
    "        \n",
    "        # train discriminator\n",
    "        d_real = discriminator.train_on_batch([encoded, half_real], reals)\n",
    "        d_synth = discriminator.train_on_batch([half_synth, generated], synths)\n",
    "                                                            \n",
    "        # train gan\n",
    "        gen_ = gan.train_on_batch([half_synth, half_real], [reals, synths])\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch %s losses: discriminator real: %.4f%%, discriminator synth: %.4f%%, generator: %.4f%%' % \n",
    "                  (i, d_real[0], d_synth[0], gen_[0]))\n",
    "        \n",
    "        d_real_metrics.append(d_real)\n",
    "        d_synth_metrics.append(d_synth)\n",
    "        g_metrics.append(gen_)\n",
    "    return d_real_metrics, d_synth_metrics, g_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_r_metrics, d_s_metrics, g_metrics = train_gan(real, gan_params['batch_size'], gan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot([metrics[0] for metrics in d_r_metrics], label='discriminator loss on reals')\n",
    "plt.plot([metrics[0] for metrics in d_s_metrics], label='discriminator loss on synths')\n",
    "plt.plot([metrics[0] for metrics in g_metrics], label='generator loss')\n",
    "plt.legend()\n",
    "plt.title('GAN losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot([metrics[1] for metrics in d_r_metrics], label='discriminator accuracy reals')\n",
    "plt.plot([metrics[1] for metrics in d_s_metrics], label='discriminator accuracy synths')\n",
    "plt.plot([metrics[1] for metrics in g_metrics], label='generator mean average error')\n",
    "plt.legend()\n",
    "plt.title('GAN performance metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_y = generator.predict(np.random.rand(num_obs, gan_params['sequence_length'], gan_cols))[:,-1,-1]\n",
    "gan_y = gan_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(gan_y, label='observed cpi')\n",
    "plt.plot(generated_y, label='gan-generated cpi')\n",
    "plt.legend()\n",
    "plt.title('Observed versus GAN-generated values for consumer price inflation in %s' % country)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rmse: %s\\nmean observed: %s\\nmean generated: %s' % (np.sqrt(mean_squared_error(gan_y, generated_y)),\n",
    "                                                           np.mean(gan_y), np.mean(generated_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration 3\n",
    "--------------------\n",
    "\n",
    "**Sequence transformer network** to generate training data, **LSTM** trained on generated data validated on the real data. See https://arxiv.org/abs/1808.06725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
