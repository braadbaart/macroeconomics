{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterfactual GAN for macroeconomic policy simulations\n",
    "\n",
    "The core idea is to use **counterfactual shifts** to simulate macroeconomic scenarios based on changes in one key indicator variable $i$ - the conditioning variable $c$. That is to say, to determine the effectiveness of for example a central bank policy measured by $\\textbf{x} = {x_1, x_2, ..., x_m}, \\textbf{x} \\subset X \\land i \\in X$ (with $\\textbf{x} = X \\setminus y$) when the exchange rate for a certain currency would $i$ would have been different from that found in the real data (counterfactually shifting $i$ to $c$ under macroeconomic scenario $s$), one needs to be able to model the effect of changes in variable $i$ on all other variables $\\textbf{x}$ in the dataset $X$.\n",
    "\n",
    "##### The Bayesian model\n",
    "\n",
    "To formalize, the generative model needs to learn both the joint probability\n",
    "\n",
    "$$ P(i_t,\\textbf{x}_t) = P(X_{t-n:t-1}|i_{t-n:t-1},\\textbf{x}_{t-n:t-1}), i \\in X, \\textbf{x} \\subset X, i \\notin \\textbf{x}$$\n",
    "\n",
    "of the conditioning variable $i$ and the conditioned variables $\\textbf{x} \\subset X$ at time $t$ conditional on previous timesteps ${t-n, ..., t-1}$, **and** the conditional probability of $\\textbf{x}$ given $i$ at time $t$ conditioned on a macroeconomic scenario $s$, where $i_t^s = c_t$,\n",
    "\n",
    "$$P(\\textbf{x}_t|c_t)$$\n",
    "\n",
    "From a Bayesian point of view, it makes sense to take the distribution in the real data as the prior. This means that the CGAN needs to\n",
    "1. Approximate the $n$ historical autoregressive priors for all variables $p(X_t|X_{t-n:t-1})$ at $X_t$ given $X_{t-n}, ..., X_{t-1}$.\n",
    "2. Infer the posterior distribution $p(X_{t-n:t-1},\\beta,\\gamma|X_t) \\propto p(X_t|X_{t-n:t-1},\\beta,\\gamma)p(X_t|X_{t-n:t-1})$, where $\\gamma$ maps contemporary relationships between variables at time $t$ and $\\beta$ models the autoregressive relationship over time for all variables, mapping $X_{t-n:t-1}$ on to $X_t$.\n",
    "3. Use the inferred posterior of the variables $\\textbf{x}$ to generate $Y = c,P(\\textbf{x}|c,s,\\beta,\\gamma)$, where $c$ is the conditioned form of $i$ shifted to simulate some explicitly programmed counterfactual scenario $s$, and $P(\\textbf{x}|c,s)$ are the values of $\\textbf{x}$ mapped onto this counterfactual scenario $s$. \n",
    "\n",
    "Given that a scenario $s$ can also be modeled as a regime, it might be good to look into RL research on the topic of policy switches in model-based RL.\n",
    "\n",
    "##### Counterfactuals\n",
    "\n",
    "From a causal inference standpoint, policy simulations should result in an estimate of either individual treatment effects ($ITE_i = Y_1^{(i)} - Y_0^{(i)}$ for treatment (1) and non-treatment (0) conditions on unit $i$), or average treatment effects ($ATE = 1/N \\sum\\limits_i y_1(i) - y_0(i)$) of the policy intervention $c$ on the other variables $\\textbf{x}$.\n",
    "\n",
    "In the language of causal analysis, the factual variable $i$ and counterfactual variable $c$ under scenario $s$ are investigated. Therefore the generator should learn to model not only the generated variables but also underlying confounders, which are hypothesized to aide in generating more realistic simulations.\n",
    "\n",
    "##### Training and inference\n",
    "\n",
    "Translating the Bayesian model to a GAN framework,\n",
    "* The generator generates datapoints conditional both on $\\textbf{A}$) autoregressive previous timesteps ${t-n, ..., t-1}$ and $\\textbf{B}$) lateral relationships between variables (longitudinal and contemporary effects), ignoring $\\textbf{C}$) the conditional variable $c$ during training.\n",
    "* The discriminator is used to train the generator to model $\\textbf{A}$) and $\\textbf{B}$) as realistically as possible.\n",
    "* During inference $c$ is shifted to simulate the counterfactual condition $\\textbf{C}$), letting the generator model the effect of $c$ on the $x \\subset X$ variables of interest.\n",
    "\n",
    "The training data will come from a training pool consisting of datapoints for some 100 or so countries, shuffled for training purposes but with country encodings to allow the model to learn different distributions. The validation data consists of a separate pool of 15 heterogeneous countries.\n",
    "\n",
    "The model learns to predict variables $\\textbf{i}_t = \\textbf{x}_t$ at time $t$ given $\\textbf{x}_{t-1}$ within the context of $\\textbf{x}_{t-n:t-1}$. For practical purposes the data feed consists of multivariate sequences of length $n-1$, and multivariate labels with length $1$. During inference, the model needs to have access to predictions at it made at previous timesteps in order to set the generated context $\\hat{\\textbf{x}}_{t-n:t-1}$.\n",
    "\n",
    "##### Loss functions\n",
    "\n",
    "The correctness of the generator can be measured by proxy by looking at **reconstruction losses** from historical data. I.e., provide it with a prompt of $m$ sequences on unseen historical country data, and determine how close it matched the real data for the remaining $n-m$ observations. Given the natural differences in country data, it might be good to take a pool of representative countries as the validation dataset. This will also help mitigate covariate shift issues (c.f. [Johansson et al. 2018](https://arxiv.org/abs/1605.03661)).\n",
    "\n",
    "Since we are dealing with counterfactuals, the loss metric cannot be measured directly from labeled data. However, a proxy of **counterfactual loss** could be to measure the proportional, variable-bound divergence of the simulated data from the real distribution, for example through Kullback-Leibler divergence, Fisher information, or optimal transport. The assumption underlying such an approach would be that of *real-world constraints* on macroeconomic data, i.e. the overall quantities of the indicator variable won't change as a result of counterfactual evidence, but their underlying distribution might.\n",
    "\n",
    "##### Counterfactual GAN\n",
    "\n",
    "The generator models $\\beta$ and $\\gamma$ through the same parameter set, as is current best practice in sequence modeling. One obvious candidate architecture that springs to mind are transformer networks, for example the [universal transformer](https://arxiv.org/abs/1807.03819) architecture. \n",
    "\n",
    "A correct measurement of the loss will need to incorporate covariate shifts across country datasets into the function. One widely-used metric to include a measure thereof in the loss function is maximum-mean discrepancy (MMD, c.f. [Gretton et al., 2012](http://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf), [Sutherland et al., 2107](https://arxiv.org/abs/1611.04488)), which can be estimated empirically over the means of the real and simulated data distributions. \n",
    "\n",
    "*// TODO: investigate loss functions for time series generation and simulation models.*\n",
    "\n",
    "##### Simulation\n",
    "\n",
    "*// TODO: implement inference layer that allows for the combination of manually conditioned timeseries data $c$ and generated data $x$.*\n",
    "\n",
    "\n",
    "###### Generalization\n",
    "\n",
    "*// TODO: develop tests and visualisations to check the generalization capabilities of the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:20,.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation_countres = ['France']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'France'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv('features/m_one/%s_features.csv' % country, sep=';', header=0)\n",
    "labels_df = pd.read_csv('features/m_one/labels_interpolated.csv', sep=';', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([features_df, labels_df.drop(columns=['date'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "[sns.lineplot(x='date', y=c, markers=True, ax=ax, label=c, data=combined_df) for c in list([country, 'lending interest rate', 'real interest rate', 'inflation', 'gross domestic savings', 'government debt service'])]\n",
    "\n",
    "xticks=ax.xaxis.get_major_ticks()\n",
    "for i in range(len(xticks)):\n",
    "    if i % 12 == 1:\n",
    "        xticks[i].set_visible(True)\n",
    "    else:\n",
    "        xticks[i].set_visible(False)\n",
    "\n",
    "ax.set_xticklabels(combined_df['date'], rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_df = combined_df[['date', 'bank capital to assets ratio', 'bank nonperforming loans', 'cereal yield',\n",
    "                               'energy imports', 'food exports', 'high-tech exports', 'inflation',\n",
    "                               'lending interest rate', 'life expectancy', 'population density', 'real interest rate',\n",
    "                               'broad money', 'exports of goods and services', 'gross domestic savings',\n",
    "                               'high-tech value added', 'household consumption expenditure',\n",
    "                               'imports of goods and services', 'listed companies', 'manufacturing value added',\n",
    "                               'r and d spend', 'services trade', 'trade', 'government debt service',\n",
    "                               'government interest payments external debt', 'government tax revenue', 'birth deaths',\n",
    "                               'broadband subscriptions', 'electricity access', 'co2 emissions',\n",
    "                               'electricity consumption', 'mobile subscriptions', 'newborns', 'overweight',\n",
    "                               'rural population', 'urban population', country]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_feature_df['label'] = base_feature_df[country].shift(periods=1)\n",
    "base_df = base_feature_df.drop(country, axis=1).fillna(0.00);\n",
    "base_df.set_index('date');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_df = base_df[['inflation']]\n",
    "sim_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = len(sim_df)\n",
    "num_variables = len(sim_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_df = base_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual generative adverserial network\n",
    "\n",
    "* The generator consists of a transformer architecture adapted for continous time series simulations.\n",
    "* The discriminator optimises for similarities in distributions across real and simulated data and corectness of the learnt distribution w.r.t. validation data.\n",
    "* Conditional time series can be fed to the architecture to investigate counterfactual scenarios through simulation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_params = {\n",
    "   'num_epochs': 1000,\n",
    "   'save_interval': 100,\n",
    "   'num_timesteps': 6,\n",
    "   'num_variables': num_variables,\n",
    "   'batch_size': 32,\n",
    "   'lr': 0.01 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_params = {\n",
    "   'num_layers': 1,\n",
    "   'd_model': 256,\n",
    "   'dff': 512,\n",
    "   'num_heads': 1,\n",
    "   'dropout_rate': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_params = {\n",
    "   'bi_lstm_units': 64,\n",
    "   'dropout_rate': 0.3,\n",
    "   'lr': 0.01\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_at_n = []\n",
    "n_to_t = []\n",
    "t_at_n_plus_one = []\n",
    "n_plus_one_to_t_plus_one = []\n",
    "cf_up_to_n = []\n",
    "cf_up_to_n_plus_one = []\n",
    "\n",
    "for i in range(int(num_obs / gan_params['num_timesteps'])):\n",
    "    n_to_t.append(sim_df[i:(i+gan_params['num_timesteps'])].values)\n",
    "    t_at_n.append(sim_df[(i+gan_params['num_timesteps']):(i+gan_params['num_timesteps']+1)].values)\n",
    "    t_at_n_plus_one.append(sim_df[(i+gan_params['num_timesteps']+1):(i+gan_params['num_timesteps']+2)].values)\n",
    "    n_plus_one_to_t_plus_one.append(sim_df[i+1:(i+gan_params['num_timesteps']+1)].values)\n",
    "    cf_up_to_n.append(counterfactual_df[i:(i+gan_params['num_timesteps']+1)].values)\n",
    "    cf_up_to_n_plus_one.append(counterfactual_df[i+1:(i+gan_params['num_timesteps']+2)].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input = np.asarray(t_at_n)\n",
    "src_input = src_input.reshape((int(num_obs / gan_params['num_timesteps']), num_variables, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_input = np.asarray(t_at_n_plus_one)\n",
    "tar_input = tar_input.reshape((int(num_obs / gan_params['num_timesteps']), num_variables, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_history_input = np.asarray(n_to_t)\n",
    "src_history_input = src_history_input.reshape((int(num_obs / gan_params['num_timesteps']),\n",
    "                                               num_variables, gan_params['num_timesteps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_history_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_history_input = np.asarray(n_plus_one_to_t_plus_one)\n",
    "tar_history_input = tar_history_input.reshape((int(num_obs / gan_params['num_timesteps']),\n",
    "                                               num_variables, gan_params['num_timesteps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_src_features = np.asarray(cf_up_to_n)\n",
    "cf_src_features = cf_src_features.reshape((int(num_obs / gan_params['num_timesteps']), gan_params['num_variables'],\n",
    "                                          gan_params['num_timesteps']+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_tar_features = np.asarray(cf_up_to_n_plus_one)\n",
    "cf_tar_features = cf_tar_features.reshape((int(num_obs / gan_params['num_timesteps']), gan_params['num_variables'],\n",
    "                                          gan_params['num_timesteps']+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The generator consists of transformer-based time series generator. A set of variables at fixed positions is used as source, and that same set of in the next timestep is used as target. Since the model is ran over continuous variables, rather than an embedding $n$ context timesteps are fed to the model encoded with relative positional embeddings. Since the autoregression of interest are learnt from the context timesteps rather than from the position of the variable in $\\textbf{x}$ (where ordering is fixed but random), some modifications had to be made to ensure the model learnt correct dependencies between variables in the multivariate set $\\textbf{x}$. Furthermore, rather than tokens, the generator needs to learn continous variables. The approach here is inspired by an application of the transformer architecture to music generation by [the Google Magenta team](https://magenta.tensorflow.org/music-transformer).\n",
    "\n",
    "The Transformer implementation in TensorFlow Keras is adapted for timeseries from [the TensorFlow transformer tutorial](https://www.tensorflow.org/beta/tutorials/text/transformer) for neural machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_encoding(variables, d_model):\n",
    "    angle_rads = get_angles(np.arange(variables)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    cosines = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = variable_encoding(gan_params['num_variables'], generator_params['dff'])\n",
    "print (encoding.shape)\n",
    "\n",
    "plt.pcolormesh(encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, generator_params['dff']))\n",
    "plt.ylabel('Variable')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "  \n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0\n",
    "        self.depth = d_model // self.num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "          \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "      \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "      \n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "      \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size) \n",
    "        v = self.split_heads(v, batch_size) \n",
    "\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "    \n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'), tf.keras.layers.Dense(d_model)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesEncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, num_variables, num_timesteps, rate=0.1):\n",
    "        super(TimeseriesEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "    \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "      \n",
    "    def call(self, inputs, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, k=x, q=inputs, mask=mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = TimeseriesEncoderLayer(512, 8, 2048, gan_params['num_variables'], gan_params['num_timesteps'])\n",
    "sample_encoder_output = sample_encoder(\n",
    "    tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1)),\n",
    "    x=tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], gan_params['num_timesteps'])),\n",
    "    training=False, mask=None)\n",
    "sample_encoder_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, num_variables, num_timesteps, rate=0.1):\n",
    "        super(TimeseriesDecoderLayer, self).__init__()\n",
    "    \n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "     \n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "      \n",
    "      \n",
    "    def call(self, inputs, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, k=x, q=inputs, mask=look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + inputs)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, k=enc_output, q=out1, mask=padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = TimeseriesDecoderLayer(512, 8, 2048, gan_params['num_variables'], gan_params['num_timesteps'])\n",
    "\n",
    "sample_decoder_output, _, _ = sample_decoder(\n",
    "    tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1)),\n",
    "    x=tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], gan_params['num_timesteps'])),\n",
    "    enc_output=sample_encoder_output, \n",
    "    training=False,\n",
    "    look_ahead_mask=None,\n",
    "    padding_mask=None)\n",
    "\n",
    "sample_decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, num_variables, num_timesteps, rate=0.1):\n",
    "        super(TimeseriesEncoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_variables = num_variables\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        self.variable_encoding = variable_encoding(num_variables, self.d_model)        \n",
    "        self.enc_layers = [TimeseriesEncoderLayer(d_model, num_heads, dff,\n",
    "                                                  num_variables, num_timesteps, rate) for _ in range(num_layers)]\n",
    "      \n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "            \n",
    "    def call(self, inputs, history, training, mask):\n",
    "        x = tf.concat([inputs, history], axis=-1)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # x += self.variable_encoding[:, :self.num_variables, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](inputs, x=x, training=training, mask=mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = TimeseriesEncoder(num_layers=2, d_model=generator_params['d_model'], num_heads=8, \n",
    "                                   dff=generator_params['dff'], num_variables=gan_params['num_variables'],\n",
    "                                   num_timesteps=gan_params['num_timesteps'])\n",
    "\n",
    "sample_encoder_output = sample_encoder(\n",
    "    tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1)),\n",
    "    history=tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], \n",
    "                               gan_params['num_timesteps'])),\n",
    "    training=False, mask=None)\n",
    "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (sample_encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, num_variables, num_timesteps, rate=0.1):\n",
    "        super(TimeseriesDecoder, self).__init__()\n",
    "    \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.num_variables = num_variables\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # self.variable_encoding = variabe_encoding(num_variables, d_model)\n",
    "        self.dec_layers = [TimeseriesDecoderLayer(d_model, num_heads, dff, num_variables, num_timesteps, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "      \n",
    "    def call(self, tar, tar_hist, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attention_weights = {}\n",
    "        \n",
    "        x = tf.concat([tar, tar_hist], axis=-1)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        # x += self.variable_encoding[:, :self.num_variables, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "    \n",
    "        for i in range(self.num_layers):\n",
    "            #x, enc_output, training, look_ahead_mask, padding_mask\n",
    "            x, block1, block2 = self.dec_layers[i](tar, x=x, enc_output=enc_output, training=training,\n",
    "                                                   look_ahead_mask=look_ahead_mask, padding_mask=padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "        \n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = TimeseriesDecoder(num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "                                   num_variables=gan_params['num_variables'],\n",
    "                                   num_timesteps=gan_params['num_timesteps'])\n",
    "\n",
    "output, attn = sample_decoder(\n",
    "    tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1)),\n",
    "    tar_hist=tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], \n",
    "                               gan_params['num_timesteps'])), \n",
    "    enc_output=sample_encoder_output, \n",
    "    training=False,\n",
    "    look_ahead_mask=None, \n",
    "    padding_mask=None)\n",
    "\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesTransformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, num_variables, num_timesteps, rate=0.1):\n",
    "        super(TimeseriesTransformer, self).__init__()\n",
    "\n",
    "        self.encoder = TimeseriesEncoder(num_layers, d_model, num_heads, dff, num_variables, num_timesteps, rate)\n",
    "        self.decoder = TimeseriesDecoder(num_layers, d_model, num_heads, dff, num_variables, num_timesteps, rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(num_variables)\n",
    "    \n",
    "    def call(self, inputs, hist=None, tar=None, tar_hist=None, training=False, enc_padding_mask=None,\n",
    "             look_ahead_mask=None, dec_padding_mask=None):\n",
    "        enc_output = self.encoder(inputs, history=hist, training=training, mask=enc_padding_mask)\n",
    "        dec_output, attention_weights = self.decoder(tar, tar_hist=hist, enc_output=enc_output, training=training,\n",
    "                                                     look_ahead_mask=look_ahead_mask, padding_mask=dec_padding_mask)\n",
    "    \n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = TimeseriesTransformer(num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "                                           num_timesteps=gan_params['num_timesteps'],\n",
    "                                           num_variables=gan_params['num_variables'])\n",
    "\n",
    "t = tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1))\n",
    "hist = tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], gan_params['num_timesteps']))\n",
    "t_plus_one = tf.random.uniform((gan_params['batch_size'], gan_params['num_variables'], 1))\n",
    "\n",
    "fn_out, _ = sample_transformer(t, hist=hist, tar=t_plus_one)\n",
    "\n",
    "fn_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the transformer model for the generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(params):\n",
    "    t = tf.keras.layers.Input(shape=(params['num_variables'], 1))\n",
    "    hist = tf.keras.layers.Input(shape=(params['num_variables'], params['num_timesteps']))\n",
    "    tar = tf.keras.layers.Input(shape=(params['num_variables'], 1))\n",
    "    tar_hist = tf.keras.layers.Input(shape=(params['num_variables'], params['num_timesteps']))\n",
    "    \n",
    "    transformer = TimeseriesTransformer(num_layers=2, d_model=params['d_model'], num_heads=params['num_heads'], \n",
    "                                        dff=params['dff'], num_variables=params['num_variables'], \n",
    "                                        num_timesteps=params['num_timesteps'])\n",
    "    output, weights = transformer(t, hist=hist, tar=tar, tar_hist=tar_hist)\n",
    "    g = tf.keras.layers.GaussianNoise(params['dropout_rate'])(output)\n",
    "    g = tf.keras.layers.Dense(params['num_variables'], activation='sigmoid')(g)\n",
    "    \n",
    "    generator = tf.keras.models.Model(inputs=[t, hist, tar, tar_hist], outputs=g, name='generator')\n",
    "    generator.summary()\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator({**gan_params, **generator_params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile('adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.fit([src_input, src_history_input, tar_input, tar_history_input], epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(params):    \n",
    "    output = tf.keras.layers.Input(shape=(params['num_variables']))\n",
    "    \n",
    "    d = tf.keras.layers.Dense(params['bi_lstm_units'], activation='tanh')(output)\n",
    "    d = tf.keras.layers.Dropout(params['dropout_rate'])(d)\n",
    "    d = tf.keras.layers.Dense(1, activation='softmax')(d)\n",
    "    \n",
    "    discriminator = tf.keras.models.Model(output, d, name=\"discriminator\")\n",
    "    discriminator.summary()\n",
    "    \n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator({**gan_params, **discriminator_params})\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cgan(generator, discriminator, params):\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    t = tf.keras.layers.Input(shape=(params['num_variables'], 1))\n",
    "    hist = tf.keras.layers.Input(shape=(params['num_variables'], params['num_timesteps']))\n",
    "    tar = tf.keras.layers.Input(shape=(params['num_variables'], 1))\n",
    "    tar_hist = tf.keras.layers.Input(shape=(params['num_variables'], params['num_timesteps']))\n",
    "    \n",
    "    generated_variables = generator([t, hist, tar, tar_hist])\n",
    "    classified = discriminator(generated_variables)\n",
    "    \n",
    "    cgan = tf.keras.models.Model([t, hist, tar, tar_hist], [generated_variables, classified], name='cgan')\n",
    "    cgan.summary()\n",
    "    return cgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "    \"generator\": \"mean_squared_error\",\n",
    "    \"discriminator\": \"binary_crossentropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = { \"generator\": 1.0, \"discriminator\": 0.5 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = build_cgan(generator, discriminator, gan_params)\n",
    "cgan.compile(loss=losses, loss_weights=loss_weights, optimizer='adam', metrics=['mse', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cgan(src, src_hist, tar, tar_hist, batch_size, params):\n",
    "    g_metrics = []\n",
    "    d_metrics = []\n",
    "    \n",
    "    half_batch = int(batch_size / 2)\n",
    "    \n",
    "    real_classified_as_real = np.ones(half_batch)\n",
    "    synth_classified_as_fake = np.zeros(half_batch)\n",
    "    discriminator_label_batch = np.concatenate((real_classified_as_real, synth_classified_as_fake))\n",
    "    \n",
    "    for i in range(params['num_epochs']):\n",
    "        # create input of real and synthetic data\n",
    "        random_index = np.random.randint(0, len(feature_data) - batch_size)\n",
    "        src_batch = src[random_index:int(random_index + half_batch)]\n",
    "        src_hist_batch = src_hist[random_index:int(random_index + half_batch)]\n",
    "        synth_features = np.random.normal(-1.0, 1.0, size=[half_batch, params['num_timesteps'], \n",
    "                                                           feature_data.shape[2]])\n",
    "        generator_input_batch = np.concatenate((real_features, synth_features))\n",
    "        \n",
    "        real_labels = label_data[random_index:int(random_index + half_batch)]\n",
    "        synth_labels = np.random.normal(-1.0, 1.0, size=[half_batch, 1, feature_data.shape[2]])\n",
    "        generator_label_batch = np.concatenate((real_labels, synth_labels))\n",
    "        \n",
    "        # apply generator\n",
    "        generated = generator.predict(generator_input_batch)\n",
    "        \n",
    "        # train discriminator\n",
    "        disc = discriminator.train_on_batch(generated)\n",
    "                                                            \n",
    "        # train gan\n",
    "        gen_ = cgan.train_on_batch([generator_input_batch, ], [generator_label_batch, discriminator_label_batch])\n",
    "        if i % 100 == 0:\n",
    "            print('Epoch %s losses: discriminator %s, generator: %s' % (i, str(disc[0]), str(gen_[0])))\n",
    "        \n",
    "        d_metrics.append(disc)\n",
    "        g_metrics.append(gen_)\n",
    "    return d_metrics, g_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_metrics, g_metrics = train_cgan(t_input, history_input, gan_params['batch_size'], gan_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot([metrics[0] for metrics in d_metrics], label='discriminator loss')\n",
    "plt.plot([metrics[0] for metrics in g_metrics], label='generator loss')\n",
    "plt.legend()\n",
    "plt.title('CGAN losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot([metrics[1] for metrics in d_metrics], label='discriminator mean squared error')\n",
    "plt.plot([metrics[1] for metrics in g_metrics], label='generator mean average error')\n",
    "plt.legend()\n",
    "plt.title('CGAN performance metrics')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_y = generator.predict(np.random.rand(num_obs, gan_params['sequence_length'], gan_cols))[:,-1,-1]\n",
    "gan_y = gan_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(gan_y, label='observed cpi')\n",
    "plt.plot(generated_y, label='gan-generated cpi')\n",
    "plt.legend()\n",
    "plt.title('Observed versus GAN-generated values for consumer price inflation in %s' % country)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rmse: %s\\nmean observed: %s\\nmean generated: %s' % (np.sqrt(mean_squared_error(gan_y, generated_y)),\n",
    "                                                           np.mean(gan_y), np.mean(generated_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "me",
   "language": "python",
   "name": "me"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
